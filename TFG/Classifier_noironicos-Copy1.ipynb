{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier and preprocessing\n",
    "\n",
    "In this notebook, the noironicos dataset will be treated, since ironicos's tweets are all ironic and we want a mixture of ironic and non ironic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ironic\n",
       "0    5446\n",
       "1    5638\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General import and load data\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "# Needed for running\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import database\n",
    "df=pd.read_csv('final_dataset.csv')\n",
    "df.groupby('ironic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Algunas personas sufren en las discos mientras tonean y se divierten a mil. #sarcasmo',\n",
       "       '@jacevedoaraya es para sostener el marcador..... #sarcasmo',\n",
       "       'Alguna de estas imágenes te sacara una sonrisa. \\n#Humor #Sarcasmo #ViveFeliz #domingo #Relax #Diversión \\nhttp://t.co/LqmV8iRO6V',\n",
       "       ..., 'David Trueba tiene twitter?',\n",
       "       '?Que opinais?: Manana, mi #dospalabras en @elconfidencial \"La unica alternativa de Rajoy: sacar a Espana del Euro\"',\n",
       "       '#SahelNOW ahora en el telediario de Pepa Bueno'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define X and Y\n",
    "X = df['tweet'].values\n",
    "y = df['ironic'].values.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical features\n",
    "The lexical features analysis will be performed by using the twitter tokenizer provided by nltk library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample statistics using NLTK\n",
    "# A transformer will be implemented\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class LexicalStats (BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract lexical features from each document\"\"\"\n",
    "    \n",
    "    def number_sentences(self, doc):\n",
    "        sentences = sent_tokenize(doc, language='spanish')\n",
    "        return len(sentences)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):\n",
    "        return [{'length': len(doc),\n",
    "                 'num_sentences': self.number_sentences(doc)}\n",
    "                for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', '_eurovision2014', 'en', '2013', 'falta', 'esdm', 'jajajajajajajajajajja', 'top', '1', 'hombre', '#', 'ironia']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_eurovision2014',\n",
       " '2013',\n",
       " 'falt',\n",
       " 'esdm',\n",
       " 'jajajajajajajajajajj',\n",
       " 'top',\n",
       " '1',\n",
       " 'hombr',\n",
       " 'ironi']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tokenizer will be defined\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def custom_tokenizer(words):\n",
    "    tokens = word_tokenize(words.lower())\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    lemmas = [stemmer.stem(t) for t in tokens]\n",
    "    stoplist = stopwords.words('spanish')\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmas_punct = [w for w in lemmas_clean if  w not in punctuation]\n",
    "    print(tokens)\n",
    "    return lemmas_punct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features\n",
    "\n",
    "ALOMEJOR HAY QUE QUITARLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use NLTK's tag set\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import collections\n",
    "\n",
    "# We can extract particular chunks (trozos, pedazos) from the sentence\n",
    "# if we use a RegExpParser. See Syntactic Processing\n",
    "def PosStats(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def stats(self, doc):\n",
    "        tokens = custom_tokenizer(doc)\n",
    "        \n",
    "        tagged = pos_tag(tokens, tagset = 'universal' )\n",
    "        counts = collections.Counter(tag for word, tag in tagged)\n",
    "        total = sum(counts.values())\n",
    "        #copy tags so that we return always the same number of features\n",
    "        pos_features = {'NOUN': 0, 'ADJ': 0, 'VERB': 0, 'ADV': 0, 'CONJ': 0, \n",
    "                        'ADP': 0, 'PRON':0, 'NUM': 0}\n",
    "        \n",
    "        pos_dic = dict((tag, float(count)/total) for tag,count in counts.items())\n",
    "        for k in pos_dic:\n",
    "            if k in pos_features:\n",
    "                pos_features[k] = pos_dic[k]\n",
    "        return pos_features\n",
    "    \n",
    "    def transform(self, docs, y=None):\n",
    "        return [self.stats(doc) for doc in docs]\n",
    "    \n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction Pipeline\n",
    "The feature extraction will be carried out by using pipelines. The defined pipelines are selected in order to extract the desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "ngrams_featurizer = Pipeline([\n",
    "  ('count_vectorizer',  CountVectorizer(ngram_range = (1, 4), encoding = 'ISO-8859-1', \n",
    "                                        tokenizer=custom_tokenizer)),\n",
    "  ('tfidf_transformer', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union Pipeline\n",
    "Now we define which features we want to extract, how to combine them and later apple machine learning in the resulting feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def Pipeline(clf):\n",
    "    pipeline = Pipeline([\n",
    "           ('features', FeatureUnion([\n",
    "                        ('lexical_stats', Pipeline([\n",
    "                                    ('stats', LexicalStats()),\n",
    "                                    ('vectors', DictVectorizer())\n",
    "                                ])),\n",
    "                        ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n",
    "                        ('ngrams', ngrams_featurizer),\n",
    "                        ('pos_stats', Pipeline([\n",
    "                                    ('pos_stats', PosStats()),\n",
    "                                    ('vectors', DictVectorizer())\n",
    "                                ])),\n",
    "                        ('lda', Pipeline([ \n",
    "                                    ('count', CountVectorizer(tokenizer=custom_tokenizer)),\n",
    "                                    ('lda',  LatentDirichletAllocation(n_topics=4, max_iter=5,\n",
    "                                                           learning_method='online', \n",
    "                                                           learning_offset=50.,\n",
    "                                                           random_state=0))\n",
    "                                ])),\n",
    "                    ])),\n",
    "\n",
    "            ('clf', clf)  # classifier\n",
    "        ])\n",
    "\n",
    "# Using KFold validation\n",
    "\n",
    "cv = KFold(X.shape[0], 2, shuffle=True, random_state=33)\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo quiero hacer el confussion matrix y f score. Para ello, necesito el training y testing dataset. Eso lo puedo hacer usando el vector X, pero el vector X tiene strings (arrays de twits) y el modelo (osea mi pipeline) transforma ese array de strings en números. Esos números, son procesados y metidos en el pipeline para darme el modelo. Entonces, como quiero hacer el f1 score, necesito un training y testing dataset. Pero, paa hacer el método pipeline.predict(X_test) necesito que X_test sea números (sino salta error que ya me ha pasado). Entonces, como hago ese test split en el pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Optimize and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K- Fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
