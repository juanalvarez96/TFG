{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier and preprocessing\n",
    "\n",
    "In this notebook, the noironicos dataset will be treated, since ironicos's tweets are all ironic and we want a mixture of ironic and non ironic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# General import and load data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "# Needed for running\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import database\n",
    "df=pd.read_csv('final_dataset.csv', encoding='utf-8', delimiter=\",\", header=0)\n",
    "df.groupby('ironic').size()\n",
    "\n",
    "# Delete rows containing nan\n",
    "df=df.dropna(subset=['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before splitting database, a shuffling action will be performed since data is not randomized.\n",
    "# That way the train and test splitting will be more balanced\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Define X and Y\n",
    "X = df['tweet'].values\n",
    "y = df['ironic'].values.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting\n",
    "# Test set will be the 25% taken randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical features\n",
    "The lexical features analysis will be performed by using the twitter tokenizer provided by nltk library.\n",
    "Important: This feature extractor is NOT used since tweets are considered to contain only one sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample statistics using NLTK\n",
    "# A transformer will be implemented\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "class LexicalStats (BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract lexical features from each document\"\"\"\n",
    "    \n",
    "    def number_sentences(self, doc):\n",
    "        sentences = sent_tokenize(doc, language='spanish')\n",
    "        return len(sentences)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):\n",
    "       \n",
    "        return [{'length': len(doc),\n",
    "                 'num_sentences': self.number_sentences(doc)}\n",
    "                \n",
    "                for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tokenizer will be defined\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def custom_tokenizer(words):\n",
    "    tokens = word_tokenize(words.lower())\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    lemmas = [stemmer.stem(t) for t in tokens]\n",
    "    stoplist = stopwords.words('spanish')\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmas_punct = [w for w in lemmas_clean if  w not in punctuation]\n",
    "    return lemmas_punct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features\n",
    "\n",
    "ALOMEJOR HAY QUE QUITARLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use NLTK's tag set\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import collections\n",
    "\n",
    "# We can extract particular chunks (trozos, pedazos) from the sentence\n",
    "# if we use a RegExpParser. See Syntactic Processing\n",
    "def PosStats(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def stats(self, doc):\n",
    "        tokens = custom_tokenizer(doc)\n",
    "        \n",
    "        tagged = pos_tag(tokens, tagset = 'universal' )\n",
    "        counts = collections.Counter(tag for word, tag in tagged)\n",
    "        total = sum(counts.values())\n",
    "        #copy tags so that we return always the same number of features\n",
    "        pos_features = {'NOUN': 0, 'ADJ': 0, 'VERB': 0, 'ADV': 0, 'CONJ': 0, \n",
    "                        'ADP': 0, 'PRON':0, 'NUM': 0}\n",
    "        \n",
    "        pos_dic = dict((tag, float(count)/total) for tag,count in counts.items())\n",
    "        for k in pos_dic:\n",
    "            if k in pos_features:\n",
    "                pos_features[k] = pos_dic[k]\n",
    "        return pos_features\n",
    "    \n",
    "    def transform(self, docs, y=None):\n",
    "        return [self.stats(doc) for doc in docs]\n",
    "    \n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction Pipeline\n",
    "The feature extraction will be carried out by using pipelines. The defined pipelines are selected in order to extract the desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "ngrams_featurizer = Pipeline([\n",
    "  ('count_vectorizer',  CountVectorizer(ngram_range = (1, 2), encoding = 'ISO-8859-1', \n",
    "                                        tokenizer=custom_tokenizer)),\n",
    "  ('tfidf_transformer', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union Pipeline\n",
    "Now we define which features we want to extract, how to combine them and later apple machine learning in the resulting feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def my_pipeline(clf):\n",
    "    pipeline = Pipeline([\n",
    "       ('features', FeatureUnion([\n",
    "                    \n",
    "                    ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n",
    "                    ('ngrams', ngrams_featurizer),\n",
    "                    #('pos_stats', Pipeline([\n",
    "                                #('pos_stats', PosStats()),\n",
    "                                #('vectors', DictVectorizer())\n",
    "                            #])),\n",
    "                    ('lda', Pipeline([ \n",
    "                                ('count', CountVectorizer(tokenizer=custom_tokenizer)),\n",
    "                                ('lda',  LatentDirichletAllocation(n_components=45, max_iter=5, # Change ntopics\n",
    "                                                       learning_method='online', \n",
    "                                                       learning_offset=50.,\n",
    "                                                       random_state=0))\n",
    "                            ])),\n",
    "                ])),\n",
    "       \n",
    "        ('clf', clf)  # classifier\n",
    "    ])\n",
    "    return pipeline\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8311   size of test set: 2771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('words', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_r...   transformer_weights=None)), ('clf', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "print(\"Size of training set: {}   size of test set: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "model = MultinomialNB(alpha=.01)\n",
    "modelNB = my_pipeline(model)\n",
    "modelNB.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1 = modelNB.predict(X_test)\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89281847708408513"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Accuracy\n",
    "metrics.accuracy_score(expected, predicted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.88388   0.89513   0.88947      1335\n",
      "          1    0.90134   0.89067   0.89597      1436\n",
      "\n",
      "avg / total    0.89293   0.89282   0.89284      2771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(expected, predicted1, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('words', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_r...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "types_of_kernels = ['linear', 'rbf', 'poly']\n",
    "\n",
    "kernel = types_of_kernels[0]\n",
    "gamma = 3.0\n",
    "\n",
    "# Create kNN model\n",
    "model = SVC(kernel=kernel, probability=True, gamma=gamma)\n",
    "modelSVC = my_pipeline(model)\n",
    "modelSVC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.93803   0.91835   0.92808      1335\n",
      "          1    0.92555   0.94359   0.93448      1436\n",
      "\n",
      "avg / total    0.93156   0.93143   0.93140      2771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted2 = modelSVC.predict(X_test)\n",
    "expected = y_test\n",
    "metrics.accuracy_score(expected, predicted2)\n",
    "print(classification_report(expected, predicted2, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kneighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('words', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_r...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=7)\n",
    "modelKnn = my_pipeline(model)\n",
    "modelKnn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.65775   0.90262   0.76097      1335\n",
      "          1    0.86155   0.56337   0.68126      1436\n",
      "\n",
      "avg / total    0.76337   0.72681   0.71967      2771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted3 = modelKnn.predict(X_test)\n",
    "expected = y_test\n",
    "metrics.accuracy_score(expected, predicted3)\n",
    "print(classification_report(expected, predicted3, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1232: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('words', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_r...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(n_jobs = -1)\n",
    "modelLR = my_pipeline(model)\n",
    "modelLR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.92425   0.90487   0.91446      1335\n",
      "          1    0.91325   0.93106   0.92207      1436\n",
      "\n",
      "avg / total    0.91855   0.91844   0.91840      2771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted4 = modelLR.predict(X_test)\n",
    "expected = y_test\n",
    "metrics.accuracy_score(expected, predicted4)\n",
    "print(classification_report(expected, predicted4, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize models\n",
    "Tune parameters of previously defined models using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'clf__alpha': 1.0}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.825 (+/-0.009) for {'clf__alpha': 0.0001}\n",
      "0.830 (+/-0.010) for {'clf__alpha': 0.001}\n",
      "0.842 (+/-0.009) for {'clf__alpha': 0.01}\n",
      "0.860 (+/-0.015) for {'clf__alpha': 0.1}\n",
      "0.867 (+/-0.012) for {'clf__alpha': 0.2}\n",
      "0.870 (+/-0.007) for {'clf__alpha': 0.3}\n",
      "0.872 (+/-0.007) for {'clf__alpha': 0.4}\n",
      "0.874 (+/-0.005) for {'clf__alpha': 0.5}\n",
      "0.875 (+/-0.004) for {'clf__alpha': 0.6}\n",
      "0.876 (+/-0.004) for {'clf__alpha': 0.7}\n",
      "0.877 (+/-0.001) for {'clf__alpha': 0.8}\n",
      "0.877 (+/-0.003) for {'clf__alpha': 0.9}\n",
      "0.877 (+/-0.004) for {'clf__alpha': 1.0}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.85      0.89      1335\n",
      "          1       0.87      0.95      0.91      1436\n",
      "\n",
      "avg / total       0.91      0.90      0.90      2771\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'clf__alpha': 0.8}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.824 (+/-0.009) for {'clf__alpha': 0.0001}\n",
      "0.830 (+/-0.010) for {'clf__alpha': 0.001}\n",
      "0.842 (+/-0.009) for {'clf__alpha': 0.01}\n",
      "0.859 (+/-0.014) for {'clf__alpha': 0.1}\n",
      "0.866 (+/-0.011) for {'clf__alpha': 0.2}\n",
      "0.868 (+/-0.007) for {'clf__alpha': 0.3}\n",
      "0.869 (+/-0.006) for {'clf__alpha': 0.4}\n",
      "0.871 (+/-0.002) for {'clf__alpha': 0.5}\n",
      "0.871 (+/-0.001) for {'clf__alpha': 0.6}\n",
      "0.872 (+/-0.001) for {'clf__alpha': 0.7}\n",
      "0.872 (+/-0.004) for {'clf__alpha': 0.8}\n",
      "0.872 (+/-0.007) for {'clf__alpha': 0.9}\n",
      "0.872 (+/-0.008) for {'clf__alpha': 1.0}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.86      0.90      1335\n",
      "          1       0.88      0.95      0.91      1436\n",
      "\n",
      "avg / total       0.91      0.91      0.91      2771\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Used alpha = .01\n",
    "parametersNB = {'clf__alpha': [.0001,.001,.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]}\n",
    "scoresNB = ['precision', 'recall']\n",
    "for score in scoresNB:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    gs_NB = GridSearchCV(modelNB,parametersNB, n_jobs=-1, scoring='%s_macro' % score)\n",
    "    gs_NB.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(gs_NB.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = gs_NB.cv_results_['mean_test_score']\n",
    "    stds = gs_NB.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, gs_NB.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, gs_NB.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs_NB=gs_NB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Best Score with MultinomialNB: %s\" % gs_NB.best_score_)\n",
    "for param_name in sorted(parametersNB.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_NB.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Optimize SVC\n",
    "\n",
    "\n",
    "parametersSVC = {'clf__C':range(1,15),'clf__gamma': np.logspace(-6, -1, 10), 'clf__kernel': ('linear','rbf'),\n",
    "                 'clf__probability':(True,False),}\n",
    "\n",
    "gs_SVC = GridSearchCV(modelSVC, parametersSVC, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs_SVC = gs_SVC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " print(\"Best Score with SVC: %s\" % gs_SVC.best_score_)\n",
    "for param_name in sorted(parametersSVC.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_SVC.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scoresSVC = ['precision', 'recall']\n",
    "\n",
    "for score in scoresSVC:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    gs_SVC = GridSearchCV(modelSVC, tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    gs_SVC.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(gs_SVC.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = gs_SVC.cv_results_['mean_test_score']\n",
    "    stds = gs_SVC.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, gs_SVC.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, gs_SVC.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parametersKN = {'clf__n_neighbors': range(1,15), 'clf__p':(1,2),'clf__algorithm':('ball_tree', 'kd_tree', 'brute')}\n",
    "\n",
    "gs_KN = GridSearchCV(modelKnn, parametersKN, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs_KN = gs_KN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " print(\"Best Score with KN: %s\" % gs_KN.best_score_)\n",
    "for param_name in sorted(parametersKN.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_KN.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'clf__n_neighbors': range(1,15), 'clf__p':(1,2),'clf__algorithm':('ball_tree', 'kd_tree', 'brute')}]\n",
    "\n",
    "scoresKNN = ['precision', 'recall']\n",
    "\n",
    "for score in scoresKNN:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    gs_KNN = GridSearchCV(modelKnn, tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    gs_KNN.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(gs_KNN.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = gs_KNN.cv_results_['mean_test_score']\n",
    "    stds = gs_KNN.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, gs_KNN.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, gs_KNN.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRgression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parametersLR = {'clf__penalty': ['l1','l2'], 'clf__tol': [0.0001,0.001,0.01,0.1], 'clf__C': range(1,15)}\n",
    "\n",
    "gs_LR = GridSearchCV(modelLR, parametersLR, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs_LR = gs_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " print(\"Best Score with LogisticRegression: %s\" % gs_LR.best_score_)\n",
    "for param_name in sorted(parametersLR.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_LR.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'clf__penalty': ['l1','l2'], 'clf__tol': [0.0001,0.001,0.01,0.1], 'clf__C': range(1,15)}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scoresLR:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    gs_LR = GridSearchCV(modelLR, tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    gs_LR.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(gs_KNN.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = gs_LR.cv_results_['mean_test_score']\n",
    "    stds = gs_LR.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, gs_KNN.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, gs_LR.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
