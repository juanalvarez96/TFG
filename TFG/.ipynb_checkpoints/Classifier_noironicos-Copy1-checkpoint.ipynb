{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier and preprocessing\n",
    "\n",
    "In this notebook, the noironicos dataset will be treated, since ironicos's tweets are all ironic and we want a mixture of ironic and non ironic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ironic</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Algunas personas sufren en las discos mientras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@jacevedoaraya es para sostener el marcador......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Alguna de estas im√°genes te sacara una sonrisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>@_Eurovision2014 en 2013 falta esdm jajajajaja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Hooo que buen padre...#Sarcasmo #GH2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>@JhoynerV  ja ja ja ja ja as√≠ o m√°s claro, cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@patronbermudez con todo respeto lo principios...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Gran rapidez todo en la UPO y no iban a ser me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>¬øQue humilde es Simeone no? #iron√≠a #llor√≥n #f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>¬øAlguien se viene a la playa conmigo? #resfria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Tantos enamorados que no son noviosüòµüíëüíîüíì\\ny tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Despues de 4 a√±os de desastres pol√≠ticos, el √∫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>@CHENCHA_MUU Por eso lo dije. Pq no hay agua. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>@RossyFloresR @caacosta1962 antes solo pelucon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>Igual, igual que @Pablo_Iglesias_  \\n#Ironia  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Dia de mi cumplea√±os todos me hablan, dia sigu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Le comentar√© receptor ingreso q ya nos avisare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>Escuchar cantar a esos que tienen aud√≠fonos y ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>‚Äú@cspatria: Termina el partido en el Samborond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Que yo estoy enamorao' prusss yo estoy es tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>Iron√≠as de la vida!! https://t.co/QPfPUJqoFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>@victor_gonzalo_ @joanconti i ara Uni√≥ si treu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Un s√°bado de pactos y celebracion #politica po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>Ser√© la √∫nica que piensa en que con todos esos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>Le hicieron creer a todos que era la ves√≠cula....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>@notihoyweb creo que les falta un poquito de i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>Yo s√©, pero te dije que pelear√≠amos üòÅ https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>#SoyFelizCuando se a donde voy. La #ironia es ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>Es de idiotas que te enteres que esa persona y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>7 mil millones de personas y tu insistes en al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Qu√© bien que juega #BRA . No lo deja jugar a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>1</td>\n",
       "      <td>La montaste cabron #SARCASMO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>1</td>\n",
       "      <td>@Casal @AnaPerezMartin ¬°Anda! ¬°Y yo que cre√≠a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>1</td>\n",
       "      <td>@danielordas_es ah√≠ esta! Q se acabe el mundo....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6304</th>\n",
       "      <td>1</td>\n",
       "      <td>Que poco twittea la gente a estas horas de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>1</td>\n",
       "      <td>@minhothefrog El Chiqui√ó2 mola mogoll√≥n lo bai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>1</td>\n",
       "      <td>Asquerosas barras de Toloun, a la c√°rcel 100 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>1</td>\n",
       "      <td>#Sarcasmo\\nNueva nada m√°s, mira nada m√°s...\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6309</th>\n",
       "      <td>1</td>\n",
       "      <td>Toma ya, que forma mas discreta de copiar un t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@sabiasundato_o: Investigaci√≥n sugiere que la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6312</th>\n",
       "      <td>1</td>\n",
       "      <td>Ah! Qu√© no se me hab√≠a perdido el m√≥vil, desde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6314</th>\n",
       "      <td>1</td>\n",
       "      <td>@LeMrRobespierre Y guillotine usted a gusto, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6315</th>\n",
       "      <td>1</td>\n",
       "      <td>Eso no fue foul, no porque no fue foulüòå #sarca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>1</td>\n",
       "      <td>@JuanMalaga2011 esto adem√°s de ilegal deber√≠a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>1</td>\n",
       "      <td>No manchen! A poco Televisa y TVAzteca les pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>1</td>\n",
       "      <td>@SarponRMCF @rauL_VCF1919 @partidodelas12 Entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>1</td>\n",
       "      <td>Argumentos f√©rreos, s√≥lidos, dignos de un cate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6321</th>\n",
       "      <td>1</td>\n",
       "      <td>@tapito72 jajajajaja me falt√≥ el #sarcasmo. Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6322</th>\n",
       "      <td>1</td>\n",
       "      <td>Si alguien tiene semillas de acetaminof√©n por ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6323</th>\n",
       "      <td>1</td>\n",
       "      <td>Para decir q votaron 10 millones?o aun env√≠an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6325</th>\n",
       "      <td>1</td>\n",
       "      <td>Dise√±o de sonrisas. #ventanas #slp #andarespot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>1</td>\n",
       "      <td>@Sandris_28 como no en bikiniüëô y grande Lola #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>1</td>\n",
       "      <td>La #naturaleza no nos invade nuestro espacio, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>1</td>\n",
       "      <td>Ultima hora! En Asamblea de la LNFG se decide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6335</th>\n",
       "      <td>1</td>\n",
       "      <td>@JuanSoler_ igualmente Majo! #sarcasmo\\n\\n#A5L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6337</th>\n",
       "      <td>1</td>\n",
       "      <td>#S√°bado \"Cr√≠a cuervos y te sacaran los ojos\" #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339</th>\n",
       "      <td>1</td>\n",
       "      <td>Con este pinche calor y uds quieren hacer veda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6340</th>\n",
       "      <td>1</td>\n",
       "      <td>No te simpatizo y aqui estas stalkeandome.\\n#I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6341</th>\n",
       "      <td>1</td>\n",
       "      <td>MIR√Å QUE BIEN LA #OPOSICION ,YA EST√ÅN CONVOCAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>1</td>\n",
       "      <td>@vicfanfatal @gaspar_hispano ¬°Claro que se pue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4864 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ironic                                               text\n",
       "0         1  Algunas personas sufren en las discos mientras...\n",
       "3         1  @jacevedoaraya es para sostener el marcador......\n",
       "4         1  Alguna de estas im√°genes te sacara una sonrisa...\n",
       "5         1  @_Eurovision2014 en 2013 falta esdm jajajajaja...\n",
       "6         1            Hooo que buen padre...#Sarcasmo #GH2015\n",
       "7         1  @JhoynerV  ja ja ja ja ja as√≠ o m√°s claro, cas...\n",
       "8         0  @patronbermudez con todo respeto lo principios...\n",
       "11        1  Gran rapidez todo en la UPO y no iban a ser me...\n",
       "12        1  ¬øQue humilde es Simeone no? #iron√≠a #llor√≥n #f...\n",
       "13        1  ¬øAlguien se viene a la playa conmigo? #resfria...\n",
       "14        0  Tantos enamorados que no son noviosüòµüíëüíîüíì\\ny tan...\n",
       "15        1  Despues de 4 a√±os de desastres pol√≠ticos, el √∫...\n",
       "16        1  @CHENCHA_MUU Por eso lo dije. Pq no hay agua. ...\n",
       "17        1  @RossyFloresR @caacosta1962 antes solo pelucon...\n",
       "18        1  Igual, igual que @Pablo_Iglesias_  \\n#Ironia  ...\n",
       "19        0  Dia de mi cumplea√±os todos me hablan, dia sigu...\n",
       "20        1  Le comentar√© receptor ingreso q ya nos avisare...\n",
       "22        0  Escuchar cantar a esos que tienen aud√≠fonos y ...\n",
       "23        1  ‚Äú@cspatria: Termina el partido en el Samborond...\n",
       "25        1  \"Que yo estoy enamorao' prusss yo estoy es tra...\n",
       "26        1       Iron√≠as de la vida!! https://t.co/QPfPUJqoFE\n",
       "27        1  @victor_gonzalo_ @joanconti i ara Uni√≥ si treu...\n",
       "28        1  Un s√°bado de pactos y celebracion #politica po...\n",
       "29        1  Ser√© la √∫nica que piensa en que con todos esos...\n",
       "30        1  Le hicieron creer a todos que era la ves√≠cula....\n",
       "31        1  @notihoyweb creo que les falta un poquito de i...\n",
       "32        1  Yo s√©, pero te dije que pelear√≠amos üòÅ https://...\n",
       "33        1  #SoyFelizCuando se a donde voy. La #ironia es ...\n",
       "34        1  Es de idiotas que te enteres que esa persona y...\n",
       "37        1  7 mil millones de personas y tu insistes en al...\n",
       "...     ...                                                ...\n",
       "6299      1  \"Qu√© bien que juega #BRA . No lo deja jugar a ...\n",
       "6300      1                       La montaste cabron #SARCASMO\n",
       "6301      1  @Casal @AnaPerezMartin ¬°Anda! ¬°Y yo que cre√≠a ...\n",
       "6302      1  @danielordas_es ah√≠ esta! Q se acabe el mundo....\n",
       "6304      1  Que poco twittea la gente a estas horas de la ...\n",
       "6305      1  @minhothefrog El Chiqui√ó2 mola mogoll√≥n lo bai...\n",
       "6306      1  Asquerosas barras de Toloun, a la c√°rcel 100 a...\n",
       "6307      1  #Sarcasmo\\nNueva nada m√°s, mira nada m√°s...\\n#...\n",
       "6309      1  Toma ya, que forma mas discreta de copiar un t...\n",
       "6311      0  \"@sabiasundato_o: Investigaci√≥n sugiere que la...\n",
       "6312      1  Ah! Qu√© no se me hab√≠a perdido el m√≥vil, desde...\n",
       "6314      1  @LeMrRobespierre Y guillotine usted a gusto, h...\n",
       "6315      1  Eso no fue foul, no porque no fue foulüòå #sarca...\n",
       "6316      1  @JuanMalaga2011 esto adem√°s de ilegal deber√≠a ...\n",
       "6318      1  No manchen! A poco Televisa y TVAzteca les pas...\n",
       "6319      1  @SarponRMCF @rauL_VCF1919 @partidodelas12 Entr...\n",
       "6320      1  Argumentos f√©rreos, s√≥lidos, dignos de un cate...\n",
       "6321      1  @tapito72 jajajajaja me falt√≥ el #sarcasmo. Sa...\n",
       "6322      1  Si alguien tiene semillas de acetaminof√©n por ...\n",
       "6323      1  Para decir q votaron 10 millones?o aun env√≠an ...\n",
       "6325      1  Dise√±o de sonrisas. #ventanas #slp #andarespot...\n",
       "6328      1  @Sandris_28 como no en bikiniüëô y grande Lola #...\n",
       "6331      1  La #naturaleza no nos invade nuestro espacio, ...\n",
       "6334      1  Ultima hora! En Asamblea de la LNFG se decide ...\n",
       "6335      1  @JuanSoler_ igualmente Majo! #sarcasmo\\n\\n#A5L...\n",
       "6337      1  #S√°bado \"Cr√≠a cuervos y te sacaran los ojos\" #...\n",
       "6339      1  Con este pinche calor y uds quieren hacer veda...\n",
       "6340      1  No te simpatizo y aqui estas stalkeandome.\\n#I...\n",
       "6341      1  MIR√Å QUE BIEN LA #OPOSICION ,YA EST√ÅN CONVOCAN...\n",
       "6344      1  @vicfanfatal @gaspar_hispano ¬°Claro que se pue...\n",
       "\n",
       "[4864 rows x 2 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General import and load data\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "# Needed for running\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import database\n",
    "df_noironicos_1=pd.read_csv('DATA/noironicos_bodies.csv')\n",
    "df_train = pd.read_csv('DATA/train_data.csv')\n",
    "df_development = pd.read_csv('DATA/development.csv')\n",
    "\n",
    "# Encode categorical variable (ironic)\n",
    "df_noironicos.loc[df_noironicos['ironic']==True, \"ironic\"] = 1\n",
    "df_noironicos.loc[df_noironicos['ironic']==False, \"ironic\"] = 0\n",
    "df_train.loc[df_train['sentiment/polarity/value']== 'NEU', 'sentiment/polarity/value'] = 0\n",
    "df_train.loc[df_train['sentiment/polarity/value']== 'NONE', 'sentiment/polarity/value'] = 0\n",
    "df_train.loc[df_train['sentiment/polarity/value']== 'N', 'sentiment/polarity/value'] = 1\n",
    "df_train.loc[df_train['sentiment/polarity/value']== 'P', 'sentiment/polarity/value'] = 1\n",
    "df_development.loc[df_development['sentiment'] == 'NONE', 'sentiment'] = 0\n",
    "df_development.loc[df_development['sentiment'] == 'NEU', 'sentiment'] = 0\n",
    "df_development.loc[df_development['sentiment'] == 'N', 'sentiment'] = 1\n",
    "df_development.loc[df_development['sentiment'] == 'P', 'sentiment'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# Drop non-used columns\n",
    "#df_noironicos.drop(['id_tweet', 'depends_image', 'depends_link', 'depends_retweet'], axis=1, inplace=True)\n",
    "df_train.drop(['tweetid', 'user', 'date', 'lang'], axis=1, inplace=True)\n",
    "df_development.drop(['tweetid', 'user', 'date', 'lang'], axis=1, inplace=True)\n",
    "\n",
    "#\n",
    "# Drop nan rows\n",
    "df_clean=df_noironicos.dropna(subset=['text'])\n",
    "df_noironicos=df_clean\n",
    "\n",
    "# Final dataset\n",
    "df_noironicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9058"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define X and Y\n",
    "X = df_noironicos['text'].values\n",
    "y = df_noironicos['ironic'].values.astype(int)\n",
    "\n",
    "df_noironicos[df_noironicos.ironic==1].size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical features\n",
    "The lexical features analysis will be performed by using the twitter tokenizer provided by nltk library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample statistics using NLTK\n",
    "# A transformer will be implemented\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class LexicalStats (BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract lexical features from each document\"\"\"\n",
    "    \n",
    "    def number_sentences(self, doc):\n",
    "        sentences = sent_tokenize(doc, language='spanish')\n",
    "        return len(sentences)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):\n",
    "        return [{'length': len(doc),\n",
    "                 'num_sentences': self.number_sentences(doc)}\n",
    "                for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', '_eurovision2014', 'en', '2013', 'falta', 'esdm', 'jajajajajajajajajajja', 'top', '1', 'hombre', '#', 'ironia']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_eurovision2014',\n",
       " '2013',\n",
       " 'falt',\n",
       " 'esdm',\n",
       " 'jajajajajajajajajajj',\n",
       " 'top',\n",
       " '1',\n",
       " 'hombr',\n",
       " 'ironi']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tokenizer will be defined\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def custom_tokenizer(words):\n",
    "    tokens = word_tokenize(words.lower())\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    lemmas = [stemmer.stem(t) for t in tokens]\n",
    "    stoplist = stopwords.words('spanish')\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmas_punct = [w for w in lemmas_clean if  w not in punctuation]\n",
    "    print(tokens)\n",
    "    return lemmas_punct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features\n",
    "\n",
    "ALOMEJOR HAY QUE QUITARLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use NLTK's tag set\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import collections\n",
    "\n",
    "# We can extract particular chunks (trozos, pedazos) from the sentence\n",
    "# if we use a RegExpParser. See Syntactic Processing\n",
    "def PosStats(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def stats(self, doc):\n",
    "        tokens = custom_tokenizer(doc)\n",
    "        \n",
    "        tagged = pos_tag(tokens, tagset = 'universal' )\n",
    "        counts = collections.Counter(tag for word, tag in tagged)\n",
    "        total = sum(counts.values())\n",
    "        #copy tags so that we return always the same number of features\n",
    "        pos_features = {'NOUN': 0, 'ADJ': 0, 'VERB': 0, 'ADV': 0, 'CONJ': 0, \n",
    "                        'ADP': 0, 'PRON':0, 'NUM': 0}\n",
    "        \n",
    "        pos_dic = dict((tag, float(count)/total) for tag,count in counts.items())\n",
    "        for k in pos_dic:\n",
    "            if k in pos_features:\n",
    "                pos_features[k] = pos_dic[k]\n",
    "        return pos_features\n",
    "    \n",
    "    def transform(self, docs, y=None):\n",
    "        return [self.stats(doc) for doc in docs]\n",
    "    \n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction Pipeline\n",
    "The feature extraction will be carried out by using pipelines. The defined pipelines are selected in order to extract the desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "ngrams_featurizer = Pipeline([\n",
    "  ('count_vectorizer',  CountVectorizer(ngram_range = (1, 4), encoding = 'ISO-8859-1', \n",
    "                                        tokenizer=custom_tokenizer)),\n",
    "  ('tfidf_transformer', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union Pipeline\n",
    "Now we define which features we want to extract, how to combine them and later apple machine learning in the resulting feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def Pipeline(clf):\n",
    "    pipeline = Pipeline([\n",
    "           ('features', FeatureUnion([\n",
    "                        ('lexical_stats', Pipeline([\n",
    "                                    ('stats', LexicalStats()),\n",
    "                                    ('vectors', DictVectorizer())\n",
    "                                ])),\n",
    "                        ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n",
    "                        ('ngrams', ngrams_featurizer),\n",
    "                        ('pos_stats', Pipeline([\n",
    "                                    ('pos_stats', PosStats()),\n",
    "                                    ('vectors', DictVectorizer())\n",
    "                                ])),\n",
    "                        ('lda', Pipeline([ \n",
    "                                    ('count', CountVectorizer(tokenizer=custom_tokenizer)),\n",
    "                                    ('lda',  LatentDirichletAllocation(n_topics=4, max_iter=5,\n",
    "                                                           learning_method='online', \n",
    "                                                           learning_offset=50.,\n",
    "                                                           random_state=0))\n",
    "                                ])),\n",
    "                    ])),\n",
    "\n",
    "            ('clf', clf)  # classifier\n",
    "        ])\n",
    "\n",
    "# Using KFold validation\n",
    "\n",
    "cv = KFold(X.shape[0], 2, shuffle=True, random_state=33)\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo quiero hacer el confussion matrix y f score. Para ello, necesito el training y testing dataset. Eso lo puedo hacer usando el vector X, pero el vector X tiene strings (arrays de twits) y el modelo (osea mi pipeline) transforma ese array de strings en n√∫meros. Esos n√∫meros, son procesados y metidos en el pipeline para darme el modelo. Entonces, como quiero hacer el f1 score, necesito un training y testing dataset. Pero, paa hacer el m√©todo pipeline.predict(X_test) necesito que X_test sea n√∫meros (sino salta error que ya me ha pasado). Entonces, como hago ese test split en el pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Optimize and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K- Fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
